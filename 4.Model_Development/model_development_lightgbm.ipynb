{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-24T10:22:52.612181Z",
     "iopub.status.busy": "2025-02-24T10:22:52.611839Z",
     "iopub.status.idle": "2025-02-24T10:23:03.357602Z",
     "shell.execute_reply": "2025-02-24T10:23:03.356714Z",
     "shell.execute_reply.started": "2025-02-24T10:22:52.612157Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:23:03.359216Z",
     "iopub.status.busy": "2025-02-24T10:23:03.35868Z",
     "iopub.status.idle": "2025-02-24T10:23:03.36364Z",
     "shell.execute_reply": "2025-02-24T10:23:03.362786Z",
     "shell.execute_reply.started": "2025-02-24T10:23:03.359192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to preprocess data\n",
    "def preprocess_data(df):\n",
    "    df.columns = df.columns.str.replace(' ', '_', regex=False)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['weekday'] = df['datetime'].dt.weekday\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:23:03.365223Z",
     "iopub.status.busy": "2025-02-24T10:23:03.364979Z",
     "iopub.status.idle": "2025-02-24T10:23:03.376755Z",
     "shell.execute_reply": "2025-02-24T10:23:03.376101Z",
     "shell.execute_reply.started": "2025-02-24T10:23:03.365205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to add lag features\n",
    "def add_lag_features(df, target_col, lags):\n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:23:03.378281Z",
     "iopub.status.busy": "2025-02-24T10:23:03.377994Z",
     "iopub.status.idle": "2025-02-24T10:23:03.388712Z",
     "shell.execute_reply": "2025-02-24T10:23:03.388084Z",
     "shell.execute_reply.started": "2025-02-24T10:23:03.378255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate metrics\n",
    "def calculate_metrics(actual, forecast):\n",
    "    mse = mean_squared_error(actual, forecast)\n",
    "    mae = mean_absolute_error(actual, forecast)\n",
    "    r2 = r2_score(actual, forecast)\n",
    "    rmse = np.sqrt(mse)\n",
    "    bias = np.mean(forecast) - np.mean(actual)\n",
    "    return {\n",
    "        'Mean Squared Error': mse,\n",
    "        'Mean Absolute Error': mae,\n",
    "        'R^2 Score': r2,\n",
    "        'Root Mean Squared Error': rmse,\n",
    "        'Bias': bias\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:23:03.389637Z",
     "iopub.status.busy": "2025-02-24T10:23:03.389392Z",
     "iopub.status.idle": "2025-02-24T10:23:03.400874Z",
     "shell.execute_reply": "2025-02-24T10:23:03.400123Z",
     "shell.execute_reply.started": "2025-02-24T10:23:03.389608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to train and forecast with LightGBM\n",
    "def lightgbm_forecast(df, target_col, train_end_date, is_log_transformed=False, n_splits=5):\n",
    "    train_data = df[df['datetime'] <= train_end_date]\n",
    "    test_data = df[df['datetime'] > train_end_date]\n",
    "\n",
    "    if train_data.empty or test_data.empty:\n",
    "        raise ValueError(\"Train or test data is empty. Please check the data split.\")\n",
    "\n",
    "    exclude_cols = ['datetime', 'index']\n",
    "\n",
    "    base_pollutant = target_col.replace('_log', '').split('_')[0]\n",
    "    station_suffix = '_'.join(target_col.replace('_log', '').split('_')[1:])\n",
    "\n",
    "    base_features = [\n",
    "        'direct_radiation_(W/mÂ²)',\n",
    "        f'RH_{station_suffix}',\n",
    "        f'TMP_{station_suffix}',\n",
    "        f'WDR_{station_suffix}',\n",
    "        f'WSP_{station_suffix}',\n",
    "        'is_festival',\n",
    "        'is_weekend',\n",
    "        f'{target_col}_lag_1',\n",
    "        f'{target_col}_lag_2',\n",
    "        f'{target_col}_lag_3',\n",
    "        f'{target_col}_lag_24',\n",
    "        f'{target_col}_lag_48',\n",
    "        f'{target_col}_lag_72',\n",
    "        'day',\n",
    "        'month',\n",
    "        'year',\n",
    "        'hour',\n",
    "        'weekday'\n",
    "    ]\n",
    "\n",
    "    selected_features = [feat for feat in base_features if feat in train_data.columns]\n",
    "\n",
    "    X_train = train_data[selected_features]\n",
    "    X_test = test_data[selected_features]\n",
    "\n",
    "    y_train = train_data[target_col]\n",
    "    y_test = test_data[target_col]\n",
    "\n",
    "    print(\"Extracted Features:\")\n",
    "    for feature in X_train.columns:\n",
    "        print(feature)\n",
    "\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 50,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8\n",
    "    }\n",
    "\n",
    "    # Cross-validation for evaluation only\n",
    "    for train_idx, val_idx in cv.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        train_data_lgb = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "        val_data_lgb = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data_lgb)\n",
    "\n",
    "        lgb.train(\n",
    "            params,\n",
    "            train_data_lgb,\n",
    "            num_boost_round=200,\n",
    "            valid_sets=[val_data_lgb],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "        )\n",
    "\n",
    "    # Final model training on full training data\n",
    "    final_train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    final_model = lgb.train(\n",
    "        params,\n",
    "        final_train_data,\n",
    "        num_boost_round=200\n",
    "    )\n",
    "\n",
    "    forecasts = final_model.predict(X_test)\n",
    "\n",
    "    if is_log_transformed:\n",
    "        forecasts = np.expm1(forecasts)\n",
    "        y_test = np.expm1(y_test)\n",
    "\n",
    "    metrics = calculate_metrics(y_test, forecasts)\n",
    "\n",
    "    return forecasts, pd.Series(y_test.values, index=test_data['datetime']), metrics, final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:23:03.402006Z",
     "iopub.status.busy": "2025-02-24T10:23:03.401709Z",
     "iopub.status.idle": "2025-02-24T10:23:03.414659Z",
     "shell.execute_reply": "2025-02-24T10:23:03.413941Z",
     "shell.execute_reply.started": "2025-02-24T10:23:03.40197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to apply log transformation\n",
    "def apply_log_transform(df, target_col):\n",
    "    df[f'{target_col}_log'] = np.log1p(df[target_col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:23:03.41569Z",
     "iopub.status.busy": "2025-02-24T10:23:03.415431Z",
     "iopub.status.idle": "2025-02-24T10:23:03.427622Z",
     "shell.execute_reply": "2025-02-24T10:23:03.426872Z",
     "shell.execute_reply.started": "2025-02-24T10:23:03.415672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to plot results\n",
    "def plot_results(actual, forecast, target_col, train_end_date='2022-12-31'):\n",
    "    if isinstance(forecast, np.ndarray):\n",
    "        forecast = pd.Series(forecast, index=actual.index)\n",
    "    \n",
    "    test_period = actual.index > pd.Timestamp(train_end_date)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(actual.index, actual, label='Actual (Observed)', color='blue', linewidth=1.5)\n",
    "    plt.plot(actual.index[test_period], forecast[test_period], label='Forecast (Predicted)', linestyle='--', color='orange', linewidth=1.5)\n",
    "    plt.axvline(x=pd.Timestamp(train_end_date), color='red', linestyle='--', label='Train/Test Split')\n",
    "    plt.title(f\"{target_col} Forecast vs Actual\", fontsize=16)\n",
    "    plt.xlabel(\"Date\", fontsize=12)\n",
    "    plt.ylabel(f'{target_col} Values', fontsize=12)\n",
    "    plt.grid(alpha=0.3, linestyle='--')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:23:03.430103Z",
     "iopub.status.busy": "2025-02-24T10:23:03.429846Z",
     "iopub.status.idle": "2025-02-24T10:23:03.438128Z",
     "shell.execute_reply": "2025-02-24T10:23:03.437302Z",
     "shell.execute_reply.started": "2025-02-24T10:23:03.430081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to save the trained model\n",
    "def save_model(model, target_col):\n",
    "    model_filename = f\"{target_col}_model_.pkl\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"Model saved as: {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:23:03.439731Z",
     "iopub.status.busy": "2025-02-24T10:23:03.439479Z",
     "iopub.status.idle": "2025-02-24T10:23:03.454218Z",
     "shell.execute_reply": "2025-02-24T10:23:03.453583Z",
     "shell.execute_reply.started": "2025-02-24T10:23:03.439714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Main workflow with and without log transformation\n",
    "def main_workflow_with_analysis(file_path, target_cols, train_end_date):\n",
    "    data = preprocess_data(pd.read_excel(file_path))\n",
    "    \n",
    "    results_no_log = {}\n",
    "    results_with_log = {}\n",
    "    \n",
    "    for target_col in target_cols:\n",
    "        print(f\"Processing {target_col}...\")\n",
    "        \n",
    "        lags = [1, 2, 3, 24, 48, 72]\n",
    "        data_lagged = add_lag_features(data.copy(), target_col, lags)\n",
    "        \n",
    "        forecast_no_log, actual_no_log, metrics_no_log, _ = lightgbm_forecast(data_lagged, target_col, train_end_date)\n",
    "        results_no_log[target_col] = {'metrics': metrics_no_log}\n",
    "        plot_results(actual_no_log, forecast_no_log, target_col, train_end_date)\n",
    "        \n",
    "        # Apply log transform\n",
    "        data_log = apply_log_transform(data.copy(), target_col)\n",
    "        data_log_lagged = add_lag_features(data_log, f'{target_col}_log', lags)\n",
    "        forecast_log, actual_log, metrics_log, model = lightgbm_forecast(data_log_lagged, f'{target_col}_log', train_end_date, is_log_transformed=True)\n",
    "        results_with_log[target_col] = {'metrics': metrics_log}\n",
    "        plot_results(actual_log, forecast_log, f\"{target_col} (Log Transformed)\", train_end_date)\n",
    "\n",
    "        # Save the log-transformed model\n",
    "        save_model(model, target_col)\n",
    "\n",
    "    return results_no_log, results_with_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:23:03.455274Z",
     "iopub.status.busy": "2025-02-24T10:23:03.455017Z",
     "iopub.status.idle": "2025-02-24T10:23:42.982364Z",
     "shell.execute_reply": "2025-02-24T10:23:42.981611Z",
     "shell.execute_reply.started": "2025-02-24T10:23:03.45525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Running the workflow with and without log transform\n",
    "\n",
    "# Path to current data\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "current_data = f\"{BASE_DIR}/2. Data Collection/MER_imputed_merged_data.xlsx\"\n",
    "\n",
    "pollutants = ['PM25_MER', 'PM10_MER', 'SO2_MER', 'O3_MER', 'NO2_MER', 'CO_MER']\n",
    "train_end_date = '2022-12-31'\n",
    "results_no_log, results_with_log = main_workflow_with_analysis(\n",
    "    file_path=current_data,\n",
    "    target_cols=pollutants,\n",
    "    train_end_date=train_end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:23:42.98411Z",
     "iopub.status.busy": "2025-02-24T10:23:42.983354Z",
     "iopub.status.idle": "2025-02-24T10:23:42.989084Z",
     "shell.execute_reply": "2025-02-24T10:23:42.988217Z",
     "shell.execute_reply.started": "2025-02-24T10:23:42.984083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[MER STATION] Results Without Log Transform-\")\n",
    "for pollutant, result in results_no_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:23:42.990277Z",
     "iopub.status.busy": "2025-02-24T10:23:42.989994Z",
     "iopub.status.idle": "2025-02-24T10:23:43.009608Z",
     "shell.execute_reply": "2025-02-24T10:23:43.008874Z",
     "shell.execute_reply.started": "2025-02-24T10:23:42.990248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[MER STATION] Results With Log Transform-\")\n",
    "for pollutant, result in results_with_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:23:43.01079Z",
     "iopub.status.busy": "2025-02-24T10:23:43.010476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Running the workflow with and without log transform\n",
    "\n",
    "# Path to current data\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "current_data = f\"{BASE_DIR}/2. Data Collection/AJM_imputed_merged_data.xlsx\"\n",
    "\n",
    "pollutants = ['PM25_AJM', 'PM10_AJM', 'SO2_AJM', 'O3_AJM', 'NO2_AJM', 'CO_AJM']\n",
    "train_end_date = '2022-12-31'\n",
    "results_no_log, results_with_log = main_workflow_with_analysis(\n",
    "    file_path=current_data,\n",
    "    target_cols=pollutants,\n",
    "    train_end_date=train_end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[AJM STATION] Results Without Log Transform-\")\n",
    "for pollutant, result in results_no_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[AJM STATION] Results With Log Transform-\")\n",
    "for pollutant, result in results_with_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Running the workflow with and without log transform\n",
    "\n",
    "# Path to current data\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "current_data = f\"{BASE_DIR}/2. Data Collection/BJU_imputed_merged_data.xlsx\"\n",
    "\n",
    "pollutants = ['PM25_BJU', 'PM10_BJU', 'SO2_BJU', 'O3_BJU', 'NO2_BJU', 'CO_BJU']\n",
    "train_end_date = '2022-12-31'\n",
    "results_no_log, results_with_log = main_workflow_with_analysis(\n",
    "    file_path=current_data,\n",
    "    target_cols=pollutants,\n",
    "    train_end_date=train_end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[BJU STATION] Results Without Log Transform-\")\n",
    "for pollutant, result in results_no_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[BJU STATION] Results With Log Transform-\")\n",
    "for pollutant, result in results_with_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Running the workflow with and without log transform\n",
    "\n",
    "# Path to current data\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "current_data = f\"{BASE_DIR}/2. Data Collection/HGM_imputed_merged_data.xlsx\"\n",
    "\n",
    "pollutants = ['PM25_HGM', 'PM10_HGM', 'SO2_HGM', 'O3_HGM', 'NO2_HGM', 'CO_HGM']\n",
    "train_end_date = '2022-12-31'\n",
    "results_no_log, results_with_log = main_workflow_with_analysis(\n",
    "    file_path=current_data,\n",
    "    target_cols=pollutants,\n",
    "    train_end_date=train_end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[HGM STATION] Results Without Log Transform-\")\n",
    "for pollutant, result in results_no_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[HGM STATION] Results With Log Transform-\")\n",
    "for pollutant, result in results_with_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Running the workflow with and without log transform\n",
    "\n",
    "# Path to current data\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "current_data = f\"{BASE_DIR}/2. Data Collection/MGH_imputed_merged_data.xlsx\"\n",
    "\n",
    "pollutants = ['PM25_MGH', 'PM10_MGH', 'SO2_MGH', 'O3_MGH', 'NO2_MGH', 'CO_MGH']\n",
    "train_end_date = '2022-12-31'\n",
    "results_no_log, results_with_log = main_workflow_with_analysis(\n",
    "    file_path=current_data,\n",
    "    target_cols=pollutants,\n",
    "    train_end_date=train_end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[MGH STATION] Results Without Log Transform-\")\n",
    "for pollutant, result in results_no_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[MGH STATION] Results With Log Transform-\")\n",
    "for pollutant, result in results_with_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Running the workflow with and without log transform\n",
    "\n",
    "# Path to current data\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "current_data = f\"{BASE_DIR}/2. Data Collection/MPA_imputed_merged_data.xlsx\"\n",
    "\n",
    "pollutants = ['PM25_MPA', 'PM10_MPA', 'SO2_MPA', 'O3_MPA', 'NO2_MPA', 'CO_MPA']\n",
    "train_end_date = '2022-12-31'\n",
    "results_no_log, results_with_log = main_workflow_with_analysis(\n",
    "    file_path=current_data,\n",
    "    target_cols=pollutants,\n",
    "    train_end_date=train_end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[MPA STATION] Results Without Log Transform-\")\n",
    "for pollutant, result in results_no_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[MPA STATION] Results With Log Transform-\")\n",
    "for pollutant, result in results_with_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Running the workflow with and without log transform\n",
    "\n",
    "# Path to current data\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "current_data = f\"{BASE_DIR}/2. Data Collection/PED_imputed_merged_data.xlsx\"\n",
    "\n",
    "pollutants = ['PM25_PED', 'PM10_PED', 'SO2_PED', 'O3_PED', 'NO2_PED', 'CO_PED']\n",
    "train_end_date = '2022-12-31'\n",
    "results_no_log, results_with_log = main_workflow_with_analysis(\n",
    "    file_path=current_data,\n",
    "    target_cols=pollutants,\n",
    "    train_end_date=train_end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[PED STATION] Results Without Log Transform-\")\n",
    "for pollutant, result in results_no_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[PED STATION] Results With Log Transform-\")\n",
    "for pollutant, result in results_with_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Running the workflow with and without log transform\n",
    "\n",
    "# Path to current data\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "current_data = f\"{BASE_DIR}/2. Data Collection/SAG_imputed_merged_data.xlsx\"\n",
    "\n",
    "pollutants = ['PM25_SAG', 'PM10_SAG', 'SO2_SAG', 'O3_SAG', 'NO2_SAG', 'CO_SAG']\n",
    "train_end_date = '2022-12-31'\n",
    "results_no_log, results_with_log = main_workflow_with_analysis(\n",
    "    file_path=current_data,\n",
    "    target_cols=pollutants,\n",
    "    train_end_date=train_end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[SAG STATION] Results Without Log Transform-\")\n",
    "for pollutant, result in results_no_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[SAG STATION] Results With Log Transform-\")\n",
    "for pollutant, result in results_with_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Running the workflow with and without log transform\n",
    "\n",
    "# Path to current data\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "current_data = f\"{BASE_DIR}/2. Data Collection/SFE_imputed_merged_data.xlsx\"\n",
    "\n",
    "pollutants = ['PM25_SFE', 'PM10_SFE', 'SO2_SFE', 'O3_SFE', 'NO2_SFE', 'CO_SFE']\n",
    "train_end_date = '2022-12-31'\n",
    "results_no_log, results_with_log = main_workflow_with_analysis(\n",
    "    file_path=current_data,\n",
    "    target_cols=pollutants,\n",
    "    train_end_date=train_end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[SFE STATION] Results Without Log Transform-\")\n",
    "for pollutant, result in results_no_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[SFE STATION] Results With Log Transform-\")\n",
    "for pollutant, result in results_with_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Running the workflow with and without log transform\n",
    "\n",
    "# Path to current data\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "current_data = f\"{BASE_DIR}/2. Data Collection/TLA_imputed_merged_data.xlsx\"\n",
    "\n",
    "pollutants = ['PM25_TLA', 'PM10_TLA', 'SO2_TLA', 'O3_TLA', 'NO2_TLA', 'CO_TLA']\n",
    "train_end_date = '2022-12-31'\n",
    "results_no_log, results_with_log = main_workflow_with_analysis(\n",
    "    file_path=current_data,\n",
    "    target_cols=pollutants,\n",
    "    train_end_date=train_end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[TLA STATION] Results Without Log Transform-\")\n",
    "for pollutant, result in results_no_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[TLA STATION] Results With Log Transform-\")\n",
    "for pollutant, result in results_with_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Running the workflow with and without log transform\n",
    "\n",
    "# Path to current data\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "current_data = f\"{BASE_DIR}/2. Data Collection/UIZ_imputed_merged_data.xlsx\"\n",
    "\n",
    "pollutants = ['PM25_UIZ', 'PM10_UIZ', 'SO2_UIZ', 'O3_UIZ', 'NO2_UIZ', 'CO_UIZ']\n",
    "train_end_date = '2022-12-31'\n",
    "results_no_log, results_with_log = main_workflow_with_analysis(\n",
    "    file_path=current_data,\n",
    "    target_cols=pollutants,\n",
    "    train_end_date=train_end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[UIZ STATION] Results Without Log Transform-\")\n",
    "for pollutant, result in results_no_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[UIZ STATION] Results With Log Transform-\")\n",
    "for pollutant, result in results_with_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Running the workflow with and without log transform\n",
    "\n",
    "# Path to current data\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "current_data = f\"{BASE_DIR}/2. Data Collection/XAL_imputed_merged_data.xlsx\"\n",
    "\n",
    "pollutants = ['PM25_XAL', 'PM10_XAL', 'SO2_XAL', 'O3_XAL', 'NO2_XAL', 'CO_XAL']\n",
    "train_end_date = '2022-12-31'\n",
    "results_no_log, results_with_log = main_workflow_with_analysis(\n",
    "    file_path=current_data,\n",
    "    target_cols=pollutants,\n",
    "    train_end_date=train_end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[XAL STATION] Results Without Log Transform-\")\n",
    "for pollutant, result in results_no_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n-[XAL STATION] Results With Log Transform-\")\n",
    "for pollutant, result in results_with_log.items():\n",
    "    print(f\"{pollutant}: {result['metrics']}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6499875,
     "sourceId": 10497912,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6615534,
     "sourceId": 10679246,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6626681,
     "sourceId": 10694372,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
